# ğŸš€ Proyecto de ExploraciÃ³n y Modelado con ğŸ­ Machine Learning ğŸ“Š

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Jupyter Notebook](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)

---

Este repositorio es tu pasaporte a un vibrante viaje por el **universo del Machine Learning** âœ¨, encapsulado en un solo notebook. AquÃ­, no solo "codificamos", Â¡sino que **creamos magia con datos**! ğŸ§™â€â™€ï¸âœ¨ Desarrollado por un equipo de entusiastas (Luis Salamanca, Javier Cerna, Vicente Solorza), este proyecto te guiarÃ¡ a travÃ©s de un flujo de trabajo de ML completo, desde la curiosidad inicial hasta la evaluaciÃ³n de modelos sÃºper potentes. ğŸ’¥

## ğŸ’– Â¿QuÃ© Dulces Sorpresas EncontrarÃ¡s? ğŸ¬

AdÃ©ntrate en el **`MLY0100_Entrega_2_Base-LuisSalamanca_JavierCerna_VicenteSolorza.ipynb`** y descubre:

### ğŸ” **AnÃ¡lisis Exploratorio de Datos (EDA)** - *Â¡DesentraÃ±ando Misterios!* ğŸ•µï¸â€â™€ï¸
SumÃ©rgete en tus datos como si fueran un mapa del tesoro. ğŸ—ºï¸ DescubrirÃ¡s:
* **Visualizaciones Hipnotizantes** ğŸ“Š: GrÃ¡ficos coloridos y dinÃ¡micos que revelan patrones ocultos.
    * Histogramas para distribuciones ğŸ“ˆ
    * GrÃ¡ficos de dispersiÃ³n para relaciones âœ¨
    * Diagramas de caja para detecciÃ³n de *outliers* ğŸ“¦
* **EstadÃ­sticas Descriptivas** ğŸ”¢: Medias, medianas, desviaciones estÃ¡ndar... Â¡tus datos no tendrÃ¡n secretos! ğŸ¤«
* **IngenierÃ­a de CaracterÃ­sticas** ğŸ‘·â€â™€ï¸: Creando nuevas variables a partir de las existentes para mejorar el rendimiento del modelo. Â¡La creatividad al poder! ğŸ’¡

### ğŸ§¼ **Preprocesamiento de Datos** - *Â¡Puliendo la Gema!* ğŸ’
Transformamos los datos brutos en una joya lista para el entrenamiento. AquÃ­ verÃ¡s:
* **Manejo de Valores AtÃ­picos (Outliers)** ğŸ—‘ï¸: TÃ©cnicas para identificar y gestionar esos puntos de datos rebeldes que pueden desequilibrar tus modelos. âš–ï¸
* **Escalado de CaracterÃ­sticas** ğŸ“: NormalizaciÃ³n y estandarizaciÃ³n para que tus algoritmos trabajen de manera Ã³ptima. Â¡AdiÃ³s a los sesgos por escalas! ğŸ¥³
* **CodificaciÃ³n de Variables CategÃ³ricas** ğŸ·ï¸: Transformando texto en nÃºmeros para que los modelos puedan entenderlo. Â¡Magia numÃ©rica! âœ¨

### ğŸ§  **Modelado Predictivo Avanzado** - *Â¡Dando Vida a la IA!* ğŸ¤–
Hemos entrenado y comparado un verdadero **ejÃ©rcito de algoritmos de clasificaciÃ³n**. Cada uno con su superpoder:
* **RegresiÃ³n LogÃ­stica** ğŸ¯: El clÃ¡sico que nunca falla, ideal para clasificaciones binarias.
* **MÃ¡quinas de Vectores de Soporte (SVC)** ğŸ§²: Encontrando el hiperplano perfecto para separar tus clases. Â¡Poderoso y elegante! ğŸ’«
* **Ãrboles de DecisiÃ³n** ğŸŒ³: Como un diagrama de flujo inteligente, fÃ¡cil de entender y visualizar.
* **Random Forest** ğŸŒ²ğŸŒ²ğŸŒ²: Â¡Un bosque entero de Ã¡rboles de decisiÃ³n trabajando juntos! La fuerza de la multitud. ğŸ’ª
* **Gradient Boosting (Â¡Potenciando el rendimiento!)** ğŸš€: Construyendo modelos secuencialmente, corrigiendo errores del anterior. Â¡PrecisiÃ³n al mÃ¡ximo! ğŸŒŸ

### âœ… **ValidaciÃ³n Cruzada (K-fold CV)** - *Â¡La Prueba Definitiva!* ğŸ›¡ï¸
No nos conformamos con una sola evaluaciÃ³n. Utilizamos una estrategia de **5-fold Cross-Validation** para garantizar la robustez y generalizaciÃ³n de nuestros modelos. Â¡Sin trucos, solo ciencia sÃ³lida! ğŸ”„

### ğŸ“Š **MÃ©tricas de EvaluaciÃ³n Clave** - *Â¡El Juicio de los Modelos!* ğŸ’¯
Vamos mÃ¡s allÃ¡ de la simple "precisiÃ³n". Analizamos mÃ©tricas cruciales para una comprensiÃ³n completa del rendimiento de cada modelo:
* **Accuracy** âœ…: Â¿CuÃ¡ntas predicciones correctas hizo el modelo en total?
* **Precision** ğŸ¯: De todas las predicciones positivas, Â¿cuÃ¡ntas fueron realmente correctas?
* **Recall** ğŸ”: De todas las instancias positivas reales, Â¿cuÃ¡ntas fue capaz de detectar el modelo?
* **F1-score** âš–ï¸: El equilibrio perfecto entre Precision y Recall. Â¡La mÃ©trica que lo tiene todo!

## ğŸ¯ **PropÃ³sito Brillante** âœ¨
Este proyecto es una **plantilla viva y respirante** para cualquier entusiasta del Machine Learning. Demuestra un **flujo de trabajo estructurado, reproducible y con las mejores prÃ¡cticas** para abordar problemas de clasificaciÃ³n. Â¡Ideal para aprender, inspirarse y lanzar tus propios proyectos de ML! ğŸš€

## ğŸ› ï¸ **El Kit de Herramientas Secreto** ğŸ§°
Construido con el **dream team** de las librerÃ­as de Python para Ciencia de Datos:
* **Python** ğŸ: El lenguaje de la ciencia de datos.
* **Pandas** ğŸ¼: Para la manipulaciÃ³n y anÃ¡lisis de datos de forma mÃ¡gica.
* **NumPy** ğŸ“: La base para operaciones numÃ©ricas de alto rendimiento.
* **Scikit-learn** ğŸ¤–: Tu mejor amigo para construir y evaluar modelos de ML.
* **Jupyter Notebook** ğŸ““: Nuestro laboratorio interactivo para experimentos de cÃ³digo y visualizaciones.

## ğŸ† **Resultados y Comparativa** - *Â¡La Tabla de Honor!* ğŸ…
Dentro del notebook, encontrarÃ¡s una **tabla comparativa clara y concisa** con las mÃ©tricas de rendimiento promedio (5-fold CV) para cada modelo. Â¡PodrÃ¡s ver quiÃ©n es el campeÃ³n! ğŸ‘‘

```python
# Ejemplo de la tabla que verÃ¡s en el notebook:
# ComparaciÃ³n de modelos (5-fold CV promedio):
#                       Accuracy  Precision  Recall  F1-score
# Model
# Logistic Regression      0.985      0.850   0.720     0.780
# SVC                      0.970      0.750   0.650     0.696
# Decision Tree            0.992      0.880   0.850     0.865
# RandomForest             0.995      0.920   0.890     0.905
# GradientBoosting         0.996      0.930   0.900     0.915
